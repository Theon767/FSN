import numpy as np
import torch
from torch.utils.data import WeightedRandomSampler



def generate_frustum(img, cam2ego, cam_intrinsic):
    device = imgs.device
    rgb_tr = imgs.permute(0,2,3,1)
    poses = sensor2keyegos
    Ks = cam_intrinsic
    N, H, W = rgb_tr.shape[:3]

    rays_o_tr = torch.zeros([N, H, W, 3], device=device)  
    rays_d_tr = torch.zeros([N, H, W, 3], device=device)   
    viewdirs_tr = torch.zeros([N, H, W, 3], device=device) 
    for cam_id in range(N):
        c2w = poses[cam_id]
        K = Ks[cam_id]
        rays_o_tmp, rays_d_tmp, viewdirs_tmp = get_rays_of_a_view(
                H=H, W=W, K=K, 
                c2w=c2w, ndc=False, 
                inverse_y=True, flip_x=False, flip_y=False
                )
        rays_o_tr[cam_id].copy_(rays_o_tmp.to(device))
        rays_d_tr[cam_id].copy_(rays_d_tmp.to(device))
        viewdirs_tr[cam_id].copy_(viewdirs_tmp.to(device))
        del rays_o_tmp, rays_d_tmp, viewdirs_tmp
    return rays_o_tr, rays_d_tr, viewdirs_tr, rgb_tr

def get_semantic(coor,seg_mask):
    coor_int = np.round(coor.cpu().numpy()).astype(int)
    #print('segmask shape',seg_mask.shape)
    return seg_mask[coor_int[:,0],coor_int[:,1]]

# def get_depth(coor, lidar_points,c2w, cam_intrinsic):
#     pass


def get_rays(i, j, K, c2w, inverse_y=True):
    if inverse_y:
        dirs = torch.stack([(i-K[0][2])/K[0][0], (j-K[1][2])/K[1][1], torch.ones_like(i)], -1)
    else:
        dirs = torch.stack([(i-K[0][2])/K[0][0], -(j-K[1][2])/K[1][1], -torch.ones_like(i)], -1)

    # Rotate ray directions from camera frame to the world frame
    rays_d = torch.sum(dirs[..., np.newaxis, :] * c2w[:3,:3], -1)  # dot product, equals to: [c2w.dot(dir) for dir in dirs]
    # Translate camera frame's origin to the world frame. It is the origin of all rays.
    rays_o = torch.tensor(c2w[:3,3]).expand(rays_d.shape)
    viewdirs = rays_d / rays_d.norm(dim=-1, keepdim=True)   
    return rays_o, rays_d, viewdirs

###coor are in image coordinates(u,v)

def pts2ray(coor, label_depth, label_seg, c2w, cam_intrinsic):
    rays_o, rays_d, viewdirs = get_rays(coor[:,0]+0.5, coor[:,1]+0.5, K=cam_intrinsic,c2w=c2w)
    return torch.cat([
        coor, label_depth.unsqueeze(1), label_seg.unsqueeze(1),  # 0-1, 2, 3
        rays_o, rays_d, viewdirs        # 4:7,7:10,10:13
        ], dim=1
    )
def pts2ray2(coor, corresponding_depth,seg_mask,c2w, cam_intrinsic):
    #print('coor shape', coor.shape)
    rays_o, rays_d, viewdirs = get_rays(coor[:,0]+0.5, coor[:,1]+0.5, K=cam_intrinsic,c2w=c2w)
    ###using lidar points interpolation to get label_depth
    label_depth=corresponding_depth
    label_seg=get_semantic(coor,seg_mask)
    # print("segmeantation shape",label_seg.shape)
    ###ray meaning, ray.shape(,13) 
    #[0:2] xy coordinates of voxel centers and its sampling points in image/pixel coordinates
    #[2:3] gt depth label, generated by performing knn on its surrounding lidar point
    #[3:4] gt semantic label, genearted by projecting to masked fcclip image
    #[4:7] centre of ray (Optical center of camera) in world coordinate
    #[7:10] directions of ray in world coordinate
    #[10:13] view directions of ray
    # print("coor shape:", coor.shape)                          # 期望：2D（如 [N, 2]）
    # print("label_depth shape:", torch.tensor(label_depth).unsqueeze(1).shape)  # 期望：2D（如 [N, 1]）
    # print("label_seg shape:", torch.tensor(label_seg).unsqueeze(1).shape)     # 期望：2D（如 [N, 1]）
    # print("rays_o shape:", rays_o.shape)                       # 期望：2D（如 [N, 3]）
    # print("rays_d shape:", rays_d.shape)                       # 期望：2D（如 [N, 3]）
    # print("viewdirs shape:", viewdirs.shape)                   # 期望：2D（如 [N, 3]）
    return torch.cat([
        coor, torch.tensor(label_depth).unsqueeze(1), torch.tensor(label_seg).unsqueeze(1),  # 0-1, 2, 3
        rays_o, rays_d, viewdirs        # 4:7,7:10,10:13
        ], dim=1
    )

def generate_rays(coors, label_depths, label_segs, c2w, intrins, max_ray_nums=0, time_ids=None, dynamic_class=None, balance_weight=None, weight_adj=0.3, weight_dyn=0.0, use_wrs=True):
    r"""NuScenes Dataset.

    This class serves as the API for experiments on the NuScenes Dataset.

    Args:
        coors (Nx2): coordinates of valid pixels in xy
        label_depths (Nx1): depth GT of pixels
        label_segs (Nx1): semantic GT of pixels
        c2w : camera to ego
        intrins : camera intrins matrix
        width : width of RGB image
        height : height of RGB image
        max_ray_nums : 
        time_ids (Tx1) : 
        balance_weight : 
        weight_adj : 
        weight_dyn : 
        use_wrs : 
    """
    # generate rays
    rays = []
    ids = []
    for time_id in time_ids:    # multi frames
        for i in time_ids[time_id]: # multi cameras of single frame
            ray = pts2ray(coors[i], label_depths[i], label_segs[i], c2w[i], intrins[i])
            rays.append(ray)
            ids.append(time_id)

    # Weighted Rays Sampling
    if not use_wrs:
        rays = torch.cat(rays, dim=0)
    else:
        weights = []
        if balance_weight is None:  # use batch data to compute balance_weight ( rather than the total dataset )
            classes = torch.cat([ray[:,3] for ray in rays])
            class_nums = torch.Tensor([0]*17)
            for class_id in range(17): 
                class_nums[class_id] += (classes==class_id).sum().item()
            balance_weight = torch.exp(0.005 * (class_nums.max() / class_nums - 1))

        for i in range(len(rays)):
            # wrs-a
            ans = 1.0 if ids[i]==0 else weight_adj
            weight_t = torch.full((rays[i].shape[0],), ans)
            if ids[i]!=0:
                mask_dynamic = (dynamic_class == rays[i][:, 3, None]).any(dim=-1)
                weight_t[mask_dynamic] = weight_dyn
            # wrs-b
            weight_b = balance_weight[rays[i][..., 3].long()]

            weight = weight_b * weight_t
            weights.append(weight)

        rays = torch.cat(rays, dim=0)
        weights = torch.cat(weights, dim=0)
        if max_ray_nums!=0 and rays.shape[0]>max_ray_nums:
            sampler = WeightedRandomSampler(weights, num_samples=max_ray_nums, replacement=False)
            rays = rays[list(sampler)]
    return rays


def generate_voxel_rays(voxel_centers, corresponding_depth, intrins, lidar2cams,lidar2imgs,seg_masks,extrinsics_R,extrinsics_T):
    r"""NuScenes Dataset.

    This class serves as the API for experiments on the FSN Dataset.

    Args:
        coors (Nx2): coordinates of valid pixels in xy
        label_depths (Nx1): depth GT of pixels
        label_segs (Nx1): semantic GT of pixels
        c2w : camera to ego
        intrins : camera intrins matrix
        width : width of RGB image
        height : height of RGB image
        max_ray_nums : 
        time_ids (Tx1) : 
        balance_weight : 
        weight_adj : 
        weight_dyn : 
        use_wrs : 
    """
    # generate rays
    rays = []
    ids = []
    extrinsics_R=np.load(extrinsics_R)
    extrinsics_T=np.load(extrinsics_T)
    for cam_num in range(len(intrins)):
        lidar2cam=lidar2cams[cam_num]
        # 提取 lidar2cam 的旋转和平移
        R_lidar2cam = lidar2cam[:3, :3]
        t_lidar2cam = lidar2cam[:3, 3]

        # 计算逆变换 cam2lidar
        R_cam2lidar = R_lidar2cam.T
        t_cam2lidar = -R_lidar2cam.T @ t_lidar2cam

        cam2lidar = np.eye(4)
        cam2lidar[:3, :3] = R_cam2lidar
        cam2lidar[:3, 3] = t_cam2lidar
        lidar2world = np.eye(4)
        lidar2world[:3, :3] = extrinsics_R.reshape(3,3)  # 旋转部分
        lidar2world[:3, 3] = extrinsics_T.reshape(-1,3)   # 平移部分

        camera2world = lidar2world @ cam2lidar
        ray=pts2ray2(voxel_centers,corresponding_depth,seg_masks[cam_num],cam_intrinsic=intrins[cam_num], c2w=camera2world)
        rays.append(ray)
    ###rays = torch.cat(rays, dim=0) #multi_frame
    return rays